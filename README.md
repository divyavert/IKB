# âœ¨ IKB â€“ Internal Knowledge Base (RAG System)

IKB is a modern **Retrieval-Augmented Generation (RAG)** system designed for answering internal company questions and accessing organizational knowledge efficiently. It provides natural language interactions with your company's documentation, allowing employees to quickly find answers to questions like _"How do I deploy the frontend in our staging environment?"_ or _"What's the process for requesting PTO?"_



## ğŸš€ Project Overview

- **Purpose**: A chatbot interface for querying internal company knowledge
- **Knowledge Source**: Vector database containing embedded documentation chunks
- **Architecture**:
  - **Frontend**: Modern React UI with Material UI components
  - **Backend**: FastAPI server handling requests and responses
  - **Embeddings**: ChromaDB for vector storage and retrieval
  - **LLM Integration**: Ollama for local inference

## ğŸ”„ Data Flow

1. User enters a question in the modern UI
2. FastAPI backend receives the request
3. ChromaDB performs semantic search to find relevant document chunks
4. Context is retrieved and processed
5. Ollama (with Mistral) generates a structured response
6. Response is displayed in the UI with markdown formatting

## ğŸ› ï¸ Tech Stack

### Backend

- **Language**: Python 3.x
- **API Framework**: FastAPI
- **Vector Database**: ChromaDB (persistent storage)
- **LLM Integration**: Ollama with Mistral model
- **Embeddings**: Sentence transformers (implicit through ChromaDB)

### Frontend

- **Framework**: React 19 with TypeScript
- **Build Tool**: Vite 7
- **UI Components**: Material UI 7
- **Styling**: SCSS with modern CSS features
- **HTTP Client**: Axios

## âœ… Features

- **Modern UI/UX**: Clean, responsive design with dark/light mode
- **Real-time Chat**: Natural conversation interface
- **Markdown Support**: Rich text formatting in responses
- **Semantic Chunking**: Intelligent document segmentation for improved retrieval accuracy
- **Context Retrieval**: Smart fetching of relevant document chunks
- **Structured Responses**: JSON-formatted answers with confidence levels
- **Automatic Updates**: Embedding updates based on document timestamps

## ğŸš€ Getting Started

### Prerequisites

- Python 3.x
- Node.js (latest LTS)
- Ollama installed locally with Mistral model

### Backend Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/IKB.git
cd IKB

# Set up Python environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start the server
cd ikb_backend
uvicorn server:app --reload --port 3000
```

### Frontend Setup

```bash
# Navigate to frontend directory
cd ikb_frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

### Embedding Data

```bash
# Navigate to embedding cron jobs directory
cd embedding-cron-jobs

# Run the ingestion pipeline
python -m injest.pipeline
```

## ğŸ”§ Configuration

The system can be configured to use different LLMs, embeddings, and data sources:

- **LLM**: Edit `llm.py` to change model parameters or switch LLM providers
- **Vector DB**: ChromaDB settings in `db.py`
- **Data Sources**: Configure document sources in `injest/fetcher.py`

## ğŸ“‚ Project Structure

```
IKB/
â”œâ”€â”€ assets/                         # Screenshots and demo videos
â”‚   â”œâ”€â”€ Chatting_With_Your_Docs.mp4
â”‚   â””â”€â”€ Screenshot*.png
â”œâ”€â”€ chroma_db/                      # Persistent vector database
â”œâ”€â”€ embedding-cron-jobs/            # Document processing pipeline
â”‚   â”œâ”€â”€ injest/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embedder.py             # Document embedding logic
â”‚   â”‚   â”œâ”€â”€ fetcher.py              # Document source connectors
â”‚   â”‚   â”œâ”€â”€ pipeline.py             # Main ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ query.py                # Query testing utilities
â”‚   â”‚   â””â”€â”€ transformer.py          # Document chunking logic
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ikb_backend/                    # FastAPI server
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db.py                       # ChromaDB integration
â”‚   â”œâ”€â”€ llm.py                      # Ollama/LLM integration
â”‚   â””â”€â”€ server.py                   # API endpoints
â””â”€â”€ ikb_frontend/                   # React frontend
    â”œâ”€â”€ public/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.scss                # SCSS styling
    â”‚   â”œâ”€â”€ App.tsx                 # Main React component
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ package.json
    â””â”€â”€ ...
```

## ğŸ§ª Key Components

### Semantic Chunking

The system implements intelligent semantic chunking to break down documents into meaningful, context-preserving segments before embedding. This approach:

- Preserves contextual integrity of information
- Increases retrieval accuracy by maintaining semantic coherence
- Optimizes chunk sizes for better vector representation
- Respects document structure (headings, paragraphs, lists)
- Ensures chunks contain complete thoughts rather than arbitrary text splits

### ChromaDB Integration

The system uses ChromaDB for persistent vector storage, allowing for efficient semantic search and automatic updates based on document timestamps.

### Ollama with Mistral

Local LLM inference is handled by Ollama using the Mistral model, providing structured JSON responses with confidence levels and context tracking.

### Material UI Frontend

The modern UI is built with Material UI 7, featuring:

- Dark/light mode toggle
- Responsive chat interface
- Markdown rendering
- Loading indicators
- Custom SCSS styling

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“¸ Demo & Screenshots

### Video Demo

A video demonstration of the IKB system is available in the assets folder:

- [Chatting With Your Docs.mp4](./assets/Chatting_With_Your_Docs.mp4)
